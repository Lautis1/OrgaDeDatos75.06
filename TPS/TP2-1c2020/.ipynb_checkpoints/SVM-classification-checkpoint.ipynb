{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [75.06 / 95.58] Organización de Datos\n",
    "## Trabajo Práctico 2: Competencia de Machine Learning\n",
    "### Grupo 18: DATAVID-20\n",
    "\n",
    "* 102732 - Bilbao, Manuel\n",
    "* 101933 - Karagoz, Filyan\n",
    "* 98684 - Markarian, Darío\n",
    "* 100901 - Stroia, Lautaro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importación general de librerias y set-up de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#!pip3 install tensorflow_text\n",
    "import tensorflow as tf\n",
    "import tensorflow_text\n",
    "import tensorflow_hub as hub \n",
    "\n",
    "\n",
    "import gensim\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.options.display.max_rows = None #mostrar todas las filas del df\n",
    "%matplotlib inline\n",
    "pd.options.display.float_format = '{:20,.2f}'.format # suprimimos la notacion cientifica en los outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set-up y limpieza de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "#Eliminar numeros de un texto\n",
    "def eliminar_numeros(text):\n",
    "    return re.sub(\"\\d+\", \"\",text)\n",
    "\n",
    "#Eliminar puntuacion\n",
    "def eliminar_puntuacion(text):\n",
    "    return re.sub(r'[^\\w\\s]','',text)\n",
    "\n",
    "#Pasar letras a minusculas\n",
    "def minusculas(text):\n",
    "    return text.lower()\n",
    "\n",
    "#Eliminar caracteres especiales\n",
    "def eliminar_caracteres(text):\n",
    "    return re.sub('[^a-zA-Z0-9 \\n\\.]', '',text)\n",
    "\n",
    "#Eliminar urls\n",
    "def eliminar_url(text):\n",
    "    url_reg = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_reg.sub(r'',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in [test,train]:\n",
    "    data['text'] = data['text'].apply(lambda x: eliminar_puntuacion(x))\n",
    "    data['text'] = data['text'].apply(lambda x: minusculas(x))\n",
    "    data['text'] = data['text'].apply(lambda x: eliminar_numeros(x))\n",
    "    data['text'] = data['text'].apply(lambda x: eliminar_caracteres(x))\n",
    "    data['text'] = data['text'].apply(lambda x: remove_stopwords(x))\n",
    "    data['text'] = data['text'].apply(lambda x: eliminar_url(x))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. SVM - Classification (hiperparametros default)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este algoritmo usa \"kernels\", los que transforman los datos de entrada en un formato especifico: toman el input de baja dimension y lo transforma en datos de una dimension mayor. Esto ayuda a mejorar el accuracy del clasificador."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Tokenizacion y Split de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Universal Sentence Encoder\n",
    "encoder = hub.load('https://tfhub.dev/google/universal-sentence-encoder-large/5')\n",
    "\n",
    "#hago reshape de los vectores encodeados y les doy formato unidimensional con el reshape [-1]\n",
    "train_tokens = [tf.reshape(encoder([line]), [-1]).numpy() for line in train.text]\n",
    "test_tokens = [tf.reshape(encoder([line]), [-1]).numpy() for line in test.text]\n",
    "\n",
    "#Divido el set de train en 80% training y 20% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_tokens, train.target, test_size = 0.2,\n",
    "                                                   random_state = 139)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Generar, entrenar y evaluar modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score 0.8168089297439265\n",
      "Precision score 0.8422876949740035\n",
      "Recall score 0.7210682492581603\n",
      "f1_score 0.7769784172661871\n"
     ]
    }
   ],
   "source": [
    "#Generamos un clasificador SVC con un kernel lineal\n",
    "classifier = svm.SVC(kernel='linear', probability=True)\n",
    "\n",
    "#Entrenamos\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "#Predecimos\n",
    "preds = classifier.predict(X_test)\n",
    "print('Accuracy score', accuracy_score(y_test, preds))\n",
    "print('Precision score', sklearn.metrics.precision_score(y_test,preds))\n",
    "print('Recall score', sklearn.metrics.recall_score(y_test, preds))\n",
    "print('f1_score', f1_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Prediccion y submit de kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = classifier.predict(test_tokens) #0.805 KAGGLE\n",
    "submit = pd.DataFrame(test['id'])\n",
    "submit['target'] = test_pred\n",
    "#submit.to_csv('SUBMITS/submission-svm.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. SVM - Classification (con RandomizedSearchCV)\n",
    "\n",
    "Con RandomizedSearchCV buscamos los mejores valores para los hiperparametros del SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Tokenizacion y Split de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Universal Sentence Encoder\n",
    "encoder2 = hub.load('https://tfhub.dev/google/universal-sentence-encoder-large/5')\n",
    "\n",
    "#hago reshape de los vectores encodeados y les doy formato unidimensional con el reshape [-1]\n",
    "train_tokens2 = [tf.reshape(encoder2([line]), [-1]).numpy() for line in train.text]\n",
    "test_tokens2 = [tf.reshape(encoder2([line]), [-1]).numpy() for line in test.text]\n",
    "\n",
    "#Divido el set de train en 80% training y 20% test\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(train_tokens2, train.target, test_size = 0.2,\n",
    "                                                   random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Generar, entrenar y evaluar modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generamos un clasificador SVC\n",
    "classifier2 = svm.SVC()\n",
    "\n",
    "#Hiperparametros\n",
    "parameter_space= {'C': [0.1, 1, 10, 100, 1000],  \n",
    "              'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n",
    "              'kernel': ['linear','sigmoid','poly','rbf']}  \n",
    "\n",
    "#RandomizedSearch para hiperparametros\n",
    "random_params = RandomizedSearchCV(classifier2, parameter_space, refit = True)\n",
    "\n",
    "#entrenamiento\n",
    "model = random_params.fit(X_train2, y_train2)\n",
    "\n",
    "#Vemos cuales son los hiperparametros que mejores resultaados dan\n",
    "model.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score 0.8194353250164149\n",
      "Precision score 0.857421875\n",
      "Recall score 0.6848673946957878\n",
      "f1_score 0.761491760624458\n"
     ]
    }
   ],
   "source": [
    "#Predecimos\n",
    "preds = model.predict(X_test2)\n",
    "print('Accuracy score', accuracy_score(y_test2, preds))\n",
    "print('Precision score', sklearn.metrics.precision_score(y_test2,preds))\n",
    "print('Recall score', sklearn.metrics.recall_score(y_test2, preds))\n",
    "print('f1_score', f1_score(y_test2, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Submit Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred2 = model.predict(test_tokens2) #0.8109 en kaggle\n",
    "submit = pd.DataFrame(test['id'])\n",
    "submit['target'] = test_pred2\n",
    "#submit.to_csv('SUBMITS/submission-svm-gridsearch.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. SVM con un poquito de feature engineering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Tokenizacion y split de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = train.copy()\n",
    "test_features = test.copy()\n",
    "\n",
    "test_features['keyword'] = test_features['keyword'].fillna('unknown').apply(lambda x: re.sub(r'%20',' ', str(x)))\n",
    "train_features['keyword'] = train_features['keyword'].fillna('unknown').apply(lambda x: re.sub(r'%20',' ', str(x)))\n",
    "\n",
    "for data in [test_features, train_features]:\n",
    "    data['tweet_len'] = data['text'].str.len()\n",
    "    data['qty_strings'] = data['text'].apply(lambda x: len(str(x).split()))\n",
    "    data['len_gt_mean'] = (data['tweet_len'] > data['tweet_len'].mean()).astype(int)\n",
    "    data['has_keyword_notempty'] = (data['keyword'] != 'unknown').astype(int)\n",
    "    data['qty_keywords'] = data['keyword'].apply(lambda x: len(str(x).split()))\n",
    "    data['qty_urls'] = data['text'].apply(lambda x: x.count('http'))\n",
    "    data['has_location_notempty'] = (data['location'] != 'unknown').astype(int)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Universal Sentence Encoder\n",
    "encoder3 = hub.load('https://tfhub.dev/google/universal-sentence-encoder-large/5')\n",
    "\n",
    "#hago reshape de los vectores encodeados y les doy formato unidimensional con el reshape [-1]\n",
    "train_tokens3 = [tf.reshape(encoder3([line]), [-1]).numpy() for line in train_features.text]\n",
    "test_tokens3 = [tf.reshape(encoder3([line]), [-1]).numpy() for line in test_features.text]\n",
    "\n",
    "train_tokens3 = pd.DataFrame(train_tokens3)\n",
    "test_tokens3 = pd.DataFrame(test_tokens3)\n",
    "\n",
    "train_tokens3['tweet_len'] = train_features['tweet_len']\n",
    "train_tokens3['qty_strings'] = train_features['qty_strings']\n",
    "train_tokens3['len_gt_mean'] = train_features['len_gt_mean']\n",
    "train_tokens3['has_keyword_notempty'] = train_features['has_keyword_notempty']\n",
    "train_tokens3['qty_keywords'] = train_features['qty_keywords']\n",
    "train_tokens3['qty_urls'] = train_features['qty_urls']\n",
    "train_tokens3['has_location_notempty'] = train_features['has_location_notempty']\n",
    "\n",
    "test_tokens3['tweet_len'] = test_features['tweet_len']\n",
    "test_tokens3['qty_strings'] = test_features['qty_strings']\n",
    "test_tokens3['len_gt_mean'] = test_features['len_gt_mean']\n",
    "test_tokens3['has_keyword_notempty'] = test_features['has_keyword_notempty']\n",
    "test_tokens3['qty_keywords'] = test_features['qty_keywords']\n",
    "test_tokens3['qty_urls'] = test_features['qty_urls']\n",
    "test_tokens3['has_location_notempty'] = test_features['has_location_notempty']\n",
    "        \n",
    "\n",
    "#Divido el set de train en 80% training y 20% test\n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(train_tokens3, train_features.target, test_size = 0.2,\n",
    "                                                   random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Generar, entrenar y evaluar modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=10, gamma=0.0001, kernel='linear')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Generamos un clasificador SVC \n",
    "classifier3 = svm.SVC(C=0.1, gamma=0.0001,kernel='linear')\n",
    " \n",
    "#Entrenamos\n",
    "classifier3.fit(X_train3, y_train3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score 0.8082731451083388\n",
      "Precision score 0.8003442340791739\n",
      "Recall score 0.7254290171606864\n",
      "f1_score 0.7610474631751228\n"
     ]
    }
   ],
   "source": [
    "#Predecimos\n",
    "preds3 = classifier3.predict(X_test3)\n",
    "print('Accuracy score', accuracy_score(y_test3, preds3))\n",
    "print('Precision score', sklearn.metrics.precision_score(y_test3,preds3))\n",
    "print('Recall score', sklearn.metrics.recall_score(y_test3, preds3))\n",
    "print('f1_score', f1_score(y_test3, preds3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Submit de Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred3 = classifier3.predict(test_tokens3) #0.80324 en kaggle\n",
    "submit = pd.DataFrame(test['id'])\n",
    "submit['target'] = test_pred3\n",
    "#submit.to_csv('SUBMITS/submission-svm-features.csv',index=False)\n",
    "\n",
    "#DIO MENOR SCORE EN KAGGLE QUE USANDO SOLO COLUMNA DE TEXT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. SVM + Keyword + Tuneo de hiperparametros\n",
    "\n",
    "**Features a usar:**keywords.\n",
    "**Tunear hiperparametros con RandomizedSearchCV**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Tokenizacion y split de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train4 = train.copy()\n",
    "test4 = test.copy()\n",
    "test4['keyword'] = test4['keyword'].fillna('unknown').apply(lambda x: re.sub(r'%20',' ', str(x)))\n",
    "train4['keyword'] = train4['keyword'].fillna('unknown').apply(lambda x: re.sub(r'%20',' ', str(x)))\n",
    "\n",
    "train4['combined_text'] = train4['text']+';'+train4['keyword']\n",
    "test4['combined_text'] = test4['text']+';'+test4['keyword']\n",
    "\n",
    "#Universal Sentence Encoder\n",
    "encoder4 = hub.load('https://tfhub.dev/google/universal-sentence-encoder-large/5')\n",
    "\n",
    "#hago reshape de los vectores encodeados y les doy formato unidimensional con el reshape [-1]\n",
    "train_tokens4 = [tf.reshape(encoder4([line]), [-1]).numpy() for line in train4.combined_text]\n",
    "test_tokens4 = [tf.reshape(encoder4([line]), [-1]).numpy() for line in test4.combined_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens4 = pd.DataFrame(train_tokens4)\n",
    "test_tokens4 = pd.DataFrame(test_tokens4)\n",
    "\n",
    "train_tokens4['tweet_len'] = train4['text'].str.len()\n",
    "test_tokens4['tweet_len'] = test4['text'].str.len()\n",
    "#Divido el set de train en 80% training y 20% test\n",
    "X_train4, X_test4, y_train4, y_test4 = train_test_split(train_tokens4, train4.target, test_size = 0.2,\n",
    "                                                   random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1, gamma=0.01, kernel='linear')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Generamos un clasificador SVC\n",
    "classifier4 = svm.SVC(C=1, gamma=0.01,kernel='linear')\n",
    "\n",
    "#Hiperparametros\n",
    "#parameter_space= {'C': [0.1, 1, 10, 100, 1000],  \n",
    "              #'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n",
    "              #'kernel': ['linear','sigmoid','poly','rbf']}  \n",
    "\n",
    "#RandomizedSearch para hiperparametros\n",
    "#random_params = RandomizedSearchCV(classifier4, parameter_space, refit = True, verbose = 3)\n",
    "\n",
    "#Estuvo mas de 8 horas corriendo y faltando 5 fits se reiniciaba el kernel\n",
    "#de jupyter siempre.\n",
    "#Viendo los mensajes de cada score, el mejor fue \"kernel=linear, gamma=0.01, C=1, score=0.818\"\n",
    "#Asi que entreno con ese score a mano.\n",
    "#entrenamiento\n",
    "classifier4.fit(X_train4, y_train4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score 0.8036769533814839\n",
      "Precision score 0.8075539568345323\n",
      "Recall score 0.7004680187207488\n",
      "f1_score 0.7502088554720133\n"
     ]
    }
   ],
   "source": [
    "#Predecimos\n",
    "preds4 = classifier4.predict(X_test4)\n",
    "print('Accuracy score', accuracy_score(y_test4, preds4))\n",
    "print('Precision score', sklearn.metrics.precision_score(y_test4,preds4))\n",
    "print('Recall score', sklearn.metrics.recall_score(y_test4, preds4))\n",
    "print('f1_score', f1_score(y_test4, preds4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Predicciones y submit de Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred4 = classifier4.predict(test_tokens4) #0.80692 en kaggle\n",
    "submit4 = pd.DataFrame(test4['id'])\n",
    "submit4['target'] = test_pred4\n",
    "#submit4.to_csv('SUBMITS/submission-svm-features-4.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
