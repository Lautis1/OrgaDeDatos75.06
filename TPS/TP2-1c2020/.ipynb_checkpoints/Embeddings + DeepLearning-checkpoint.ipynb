{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [75.06 / 95.58] Organización de Datos\n",
    "## Trabajo Práctico 2: Competencia de Machine Learning\n",
    "### Grupo 18: DATAVID-20\n",
    "\n",
    "* 102732 - Bilbao, Manuel\n",
    "* 101933 - Karagoz, Filyan\n",
    "* 98684 - Markarian, Darío\n",
    "* 100901 - Stroia, Lautaro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importación general de librerias y set-up de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re as re\n",
    "import os\n",
    "\n",
    "#Para instalar gensim\n",
    "#! pip3 install gensim\n",
    "import gensim\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "\n",
    "#Instalar tensorflow\n",
    "#!pip3 install tensorflow\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.options.display.max_rows = None #mostrar todas las filas del df\n",
    "%matplotlib inline\n",
    "plt.style.use('default') # haciendo los graficos un poco mas bonitos en matplotlib\n",
    "plt.rcParams['figure.figsize'] = (20, 10)\n",
    "sns.set(style=\"whitegrid\") # seteando tipo de grid en seaborn\n",
    "pd.options.display.float_format = '{:20,.2f}'.format # suprimimos la notacion cientifica en los outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set = pd.read_csv('test.csv')\n",
    "train_set = pd.read_csv('train.csv')\n",
    "train_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpieza de datos\n",
    "**Definicion de funciones auxiliares**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminar numeros de un texto\n",
    "def eliminar_numeros(text):\n",
    "    return re.sub(\"\\d+\", \"\",text)\n",
    "\n",
    "#Eliminar puntuacion\n",
    "def eliminar_puntuacion(text):\n",
    "    return re.sub(r'[^\\w\\s]','',text)\n",
    "\n",
    "#Pasar letras a minusculas\n",
    "def minusculas(text):\n",
    "    return text.lower()\n",
    "\n",
    "#Eliminar caracteres especiales\n",
    "def eliminar_caracteres(text):\n",
    "    return re.sub('[^a-zA-Z0-9 \\n\\.]', '',text)\n",
    "\n",
    "#Eliminar urls\n",
    "def eliminar_url(text):\n",
    "    url_reg = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_reg.sub(r'',text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Word Embeddings para NLP\n",
    "\n",
    "### Obtenemos un set de vectores de palabras pre-entrenados de:\n",
    "#### http://nlp.stanford.edu/data/wordvecs/glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! wget -P ~ 'http://nlp.stanford.edu/data/wordvecs/glove.6B.zip' && unzip ~/glove.6B.zip -d ~/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = {}\n",
    "with open(os.environ['HOME']+'/glove.6B.200d.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], \"float32\")\n",
    "        word2vec[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separamos la variable a predecir\n",
    "Y = train_set['target']\n",
    "train_set = train_set.drop('target',axis=1)\n",
    "\n",
    "#Concateno sets de test y train para entrenar con todos los tweets de ambos sets\n",
    "data = pd.concat([train_set,test_set],axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Procesado de datos -> aplicar funciones de limpieza**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['text'].apply(lambda x: eliminar_puntuacion(x))\n",
    "data['text'] = data['text'].apply(lambda x: minusculas(x))\n",
    "data['text'] = data['text'].apply(lambda x: eliminar_numeros(x))\n",
    "data['text'] = data['text'].apply(lambda x: eliminar_caracteres(x))\n",
    "data['text'] = data['text'].apply(lambda x: remove_stopwords(x))\n",
    "data['text'] = data['text'].apply(lambda x: eliminar_url(x))\n",
    "\n",
    "text_data = data['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separo los tweets en palabras (tokens)\n",
    "tokenizer = Tokenizer()\n",
    "#Actualiza vocab interno basado en una lista de textos (text_data)\n",
    "tokenizer.fit_on_texts(text_data) \n",
    "word2index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27497"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cantidad de palabras distintas\n",
    "len(word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5334, 627, 116, 1653, 3357],\n",
       " [56, 88, 590, 7979, 7980, 1172],\n",
       " [1321, 1245, 1945, 546, 7981, 1515, 131, 1945, 546, 1408, 1017],\n",
       " [7, 4076, 1055, 131, 1408, 22],\n",
       " [20, 1246, 150, 5335, 2172, 137, 1055, 7982, 84]]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Armo secuencias de enteros a partir de los tokens\n",
    "secuencias = tokenizer.texts_to_sequences(text_data)\n",
    "secuencias[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0, 5334,  627,  116, 1653, 3357],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,   56,   88,  590, 7979, 7980, 1172],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0, 1321,\n",
       "        1245, 1945,  546, 7981, 1515,  131, 1945,  546, 1408, 1017],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    7, 4076, 1055,  131, 1408,   22],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,   20, 1246,  150, 5335, 2172,  137, 1055, 7982,   84]],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Podemos observar que los vectores formados son de distinta longitud, por lo que habia\n",
    "#que llevarlos a todos a la misma dimension agregando un padding\n",
    "data_same_dim = tensorflow.keras.preprocessing.sequence.pad_sequences(secuencias,)\n",
    "data_same_dim[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10876, 21)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dimension de la matriz. Acordar que tenemos ambos datasets (test y train) concatenados, por eso\n",
    "#tantas filas\n",
    "data_same_dim.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_test = data_same_dim[train_set.shape[0]:]\n",
    "padded_train = data_same_dim[:train_set.shape[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27498, 200)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Armo matriz de embeddings\n",
    "embeddings = np.zeros((len(word2index)+1,200))\n",
    "vec = []\n",
    "for word, i in word2index.items():\n",
    "    try:\n",
    "       embeddings[i] = word2vec[word]\n",
    "    except KeyError:\n",
    "        continue\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.55270004e-01,  3.36780012e-01, -5.23590028e-01, -2.40370005e-01,\n",
       "        1.05619997e-01,  1.18989997e-01, -5.52529991e-01,  3.66450012e-01,\n",
       "       -4.06459987e-01,  3.73580009e-01, -2.14589998e-01,  5.29079974e-01,\n",
       "        4.40459996e-01,  8.75909999e-02, -1.44730002e-01, -1.64940000e-01,\n",
       "       -2.73649991e-01,  2.56119996e-01, -5.50870001e-02,  9.07370001e-02,\n",
       "        1.82710007e-01,  2.52329993e+00,  2.40480006e-01, -3.24369997e-01,\n",
       "        5.53879976e-01, -2.04510003e-01,  1.98369995e-01, -1.71360001e-01,\n",
       "       -1.49820000e-01,  1.20710000e-01,  9.07389969e-02, -7.63079971e-02,\n",
       "       -4.71910000e-01,  2.12339997e-01, -3.11740011e-01, -6.76829964e-02,\n",
       "       -2.80149996e-01, -5.18589988e-02, -5.04290015e-02,  3.36719990e-01,\n",
       "       -1.72470003e-01, -7.40220025e-02, -1.03090003e-01,  3.96609992e-01,\n",
       "       -1.92629993e-01, -7.38490000e-02,  8.28279972e-01, -4.21000004e-01,\n",
       "       -9.29619968e-02,  5.76099992e-01, -1.36020005e-01,  1.32479995e-01,\n",
       "       -6.70439973e-02,  9.01850015e-02,  3.63029987e-01,  3.32850009e-01,\n",
       "       -2.26199999e-01, -1.87480003e-01, -1.32229999e-01,  4.42059994e-01,\n",
       "        2.35770002e-01, -8.12759995e-02, -4.24340010e-01,  3.24030012e-01,\n",
       "       -1.78409994e-01,  3.47010009e-02, -4.57249999e-01,  3.64899993e-01,\n",
       "        3.50629985e-01, -6.69790013e-03,  7.29160011e-01,  9.96989980e-02,\n",
       "       -3.59899998e-01,  3.79900008e-01, -5.09249985e-01, -2.28489995e-01,\n",
       "       -3.51080000e-01, -1.49059996e-01, -5.96719980e-01, -3.93669993e-01,\n",
       "       -9.04939994e-02, -1.89110003e-02, -1.35720000e-01,  1.06749997e-01,\n",
       "        1.60119995e-01,  9.08539966e-02, -2.06479996e-01,  2.30749995e-01,\n",
       "        5.38030028e-01, -1.46099997e+00,  9.12119970e-02, -3.59940007e-02,\n",
       "        8.48129988e-02,  4.27220017e-02, -3.51630002e-01,  7.43520036e-02,\n",
       "        1.09030001e-01, -4.45840001e-01, -3.55430007e-01, -3.61559987e-01,\n",
       "        3.33570004e-01,  1.55699998e-01, -2.81300008e-01,  1.17749996e-01,\n",
       "        4.07940000e-01,  7.75799975e-02, -2.65410006e-01,  1.45780003e+00,\n",
       "       -5.94270006e-02,  2.35379994e-01, -5.94149984e-04, -2.92430013e-01,\n",
       "        2.65659988e-01, -1.22259997e-01,  1.72069997e-01,  4.20770012e-02,\n",
       "       -1.29490003e-01,  1.14589997e-01, -6.91739976e-01, -3.09090000e-02,\n",
       "        4.56970006e-01,  5.15470028e-01,  5.78159988e-01,  1.26609996e-01,\n",
       "        1.66549996e-01, -5.93680024e-01,  4.68580015e-02,  5.13409972e-01,\n",
       "        2.13159993e-01, -2.83179998e-01, -5.09320021e-01,  9.22209993e-02,\n",
       "        8.09400007e-02, -4.52859998e-01,  3.95400003e-02, -2.73350000e-01,\n",
       "        1.08889997e-01, -1.11400001e-01, -1.29319996e-01, -4.31039989e-01,\n",
       "        7.00609982e-02, -5.55349998e-02,  2.11820006e-01, -1.43429995e-01,\n",
       "        1.15499997e+00,  1.48990005e-01,  1.35130003e-01, -7.45540023e-01,\n",
       "        2.73290008e-01,  1.67520002e-01,  3.76569986e-01,  1.44960001e-01,\n",
       "       -6.37730002e-01, -1.45140007e-01,  1.43160000e-01, -1.37549996e-01,\n",
       "       -1.36050001e-01,  5.52630015e-02, -7.86829963e-02,  5.90890013e-02,\n",
       "        2.82639991e-02,  3.29919994e-01,  5.31009994e-02, -1.79839998e-01,\n",
       "        2.94140011e-01, -1.35560006e-01,  8.94910023e-02,  2.26280004e-01,\n",
       "       -5.86280003e-02,  1.14440002e-01, -3.29450011e-01, -4.28199992e-02,\n",
       "        3.36540014e-01,  5.82620025e-01,  4.86509986e-02,  2.43949994e-01,\n",
       "       -1.07759997e-01, -1.95810005e-01,  2.99910009e-02,  4.66670007e-01,\n",
       "        8.41189981e-01, -4.85450000e-01, -4.99729991e-01,  3.83830011e-01,\n",
       "        2.43190005e-02, -3.08880005e-02, -8.56669992e-02,  5.09829998e-01,\n",
       "       -1.98899992e-02, -4.42459993e-03, -2.90270001e-01, -2.57549994e-02,\n",
       "       -6.80460036e-02,  3.45360011e-01,  2.45350003e-01, -2.82860011e-01,\n",
       "       -4.91239995e-01, -5.15660010e-02,  3.18349987e-01,  3.48439991e-01])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Mostrando un ejemplo\n",
    "embeddings[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ahora, procedemos a entrenar esta matriz de embeddings mediante algoritmo de deep learning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 21, 200)           5499600   \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 4200)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 4201      \n",
      "=================================================================\n",
      "Total params: 5,503,801\n",
      "Trainable params: 4,201\n",
      "Non-trainable params: 5,499,600\n",
      "_________________________________________________________________\n",
      "None\n",
      "Accuracy: 92.263234\n"
     ]
    }
   ],
   "source": [
    "#Definimos el modelo secuencial\n",
    "model1 = keras.Sequential()\n",
    "embedding = keras.layers.Embedding(len(word2index)+1, 200, weights=[embeddings], input_length= data_same_dim.shape[1],\n",
    "              trainable=False)\n",
    "model1.add(embedding)\n",
    "model1.add(keras.layers.Flatten())\n",
    "#Le agrego 1 red neuronal a la capa \"oculta\"\n",
    "model1.add(keras.layers.Dense(1, activation='sigmoid')) #Sigmoidea para mapear los resultados a un rango (0,1)\n",
    "#Probe agregando mas capas con 1 red neuronal y el resultado no cambia.\n",
    "\n",
    "#Compilamos el modelo1\n",
    "model1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#Resumen del modelo 1\n",
    "print(model1.summary())\n",
    "\n",
    "#Entrenamos el modelo\n",
    "model1.fit(padded_train,Y,epochs=120, verbose=0) #a mas epochs, mejor accuracy\n",
    "\n",
    "#Evaluamos el 1er modelo\n",
    "loss, accuracy = model1.evaluate(padded_train, Y, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target\n",
       "0   0       1\n",
       "1   2       1\n",
       "2   3       0\n",
       "3   9       1\n",
       "4  11       1"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Preparando el submit de kaggle\n",
    "submit = pd.DataFrame(test_set['id'],columns=['id'])\n",
    "predictions = model1.predict(padded_test)\n",
    "submit['target'] = predictions\n",
    "submit['target'] = round(submit['target']).astype('int')\n",
    "submit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#submit.to_csv('embeddings+DL.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
