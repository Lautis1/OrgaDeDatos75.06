{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [75.06 / 95.58] Organización de Datos\n",
    "## Trabajo Práctico 2: Competencia de Machine Learning\n",
    "### Grupo 18: DATAVID-20\n",
    "\n",
    "* 102732 - Bilbao, Manuel\n",
    "* 101933 - Karagoz, Filyan\n",
    "* 98684 - Markarian, Darío\n",
    "* 100901 - Stroia, Lautaro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La clasificación de sentimientos se encarga de tomar una parte de un texto y decir si a la persona que lo redactó le gusto o no el tema de lo que está hablando. En este caso, lo podemos adaptar a nuestro problema de decidir si un tweet escrito por alguien describe un suceso real o falso.\n",
    "\n",
    "**Nos vamos a enfocar en:**\n",
    "*  Construir una Red Neuronal profunda para la clasificacion.\n",
    "*  Entrenar el modelo con Word Embeddings (usando Word2Vec).\n",
    "\n",
    "**El proceso es, mas o menos, el siguiente:**\n",
    "\n",
    "Tweets -> Embeddings -> Deep RRNN -> Red Fully connected -> Funcion de activacion (Sigmoidea en este caso) -> Target (1 o 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re as re\n",
    "import os\n",
    "\n",
    "import gensim\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, GRU\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.options.display.max_rows = None #mostrar todas las filas del df\n",
    "pd.options.display.float_format = '{:20,.2f}'.format # suprimimos la notacion cientifica en los outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primer modelo.\n",
    "\n",
    "Dejando a cargo a Keras de generar los embeddings de los tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminar numeros de un texto\n",
    "def eliminar_numeros(text):\n",
    "    return re.sub(\"\\d+\", \"\",text)\n",
    "\n",
    "#Eliminar puntuacion\n",
    "def eliminar_puntuacion(text):\n",
    "    return re.sub(r'[^\\w\\s]','',text)\n",
    "\n",
    "#Pasar letras a minusculas\n",
    "def minusculas(text):\n",
    "    return text.lower()\n",
    "\n",
    "#Eliminar caracteres especiales\n",
    "def eliminar_caracteres(text):\n",
    "    return re.sub('[^a-zA-Z0-9 \\n\\.]', '',text)\n",
    "\n",
    "#Eliminar urls\n",
    "def eliminar_url(text):\n",
    "    url_reg = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_reg.sub(r'',text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set up y split de datos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5709,)\n",
      "(1904,)\n"
     ]
    }
   ],
   "source": [
    "test_set = pd.read_csv('test.csv')\n",
    "train_set = pd.read_csv('train.csv')\n",
    "\n",
    "for data in [test_set,train_set]:\n",
    "    data['text'] = data['text'].apply(lambda x: eliminar_puntuacion(x))\n",
    "    data['text'] = data['text'].apply(lambda x: minusculas(x))\n",
    "    data['text'] = data['text'].apply(lambda x: eliminar_numeros(x))\n",
    "    data['text'] = data['text'].apply(lambda x: eliminar_caracteres(x))\n",
    "    data['text'] = data['text'].apply(lambda x: remove_stopwords(x))\n",
    "    data['text'] = data['text'].apply(lambda x: eliminar_url(x))  \n",
    "\n",
    "X = train_set['text'] #features\n",
    "y = train_set['target'] #variable a predecir\n",
    "\n",
    "#Me quedo con el 75% del set para entrenar, y el otro 25% para testear\n",
    "X_train,X_valid,y_train,y_valid = train_test_split(X, y, test_size=0.25, random_state=1)\n",
    "print(X_train.shape)\n",
    "print(X_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenizacion de los datos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "data_text = pd.concat([X,test_set.text])\n",
    "tokenizer.fit_on_texts(data_text)\n",
    "\n",
    "#longitud para armar los textos con un pad para que tengan la misma longitud\n",
    "max_len = max([len(text.split()) for text in data_text])\n",
    "\n",
    "#Cantidad de vocablos\n",
    "vocab_size = len(tokenizer.word_index)+1\n",
    "\n",
    "X_train_tokens = tokenizer.texts_to_sequences(X_train)\n",
    "X_valid_tokens = tokenizer.texts_to_sequences(X_valid)\n",
    "\n",
    "X_train_padded = pad_sequences(X_train_tokens, maxlen=max_len, padding='post')\n",
    "X_valid_padded = pad_sequences(X_valid_tokens, maxlen=max_len, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Construyendo el modelo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 21, 100)           2749800   \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 32)                12864     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,762,697\n",
      "Trainable params: 2,762,697\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Defino el tamaño que van a tener los embeddings\n",
    "EMBEDDING_SIZE = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, EMBEDDING_SIZE,input_length = max_len))\n",
    "model.add(GRU(units=32, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Entrenamos el modelo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "45/45 - 3s - loss: 0.6837 - accuracy: 0.5689 - val_loss: 0.6821 - val_accuracy: 0.5730\n",
      "Epoch 2/25\n",
      "45/45 - 2s - loss: 0.6804 - accuracy: 0.5695 - val_loss: 0.6611 - val_accuracy: 0.5746\n",
      "Epoch 3/25\n",
      "45/45 - 2s - loss: 0.4407 - accuracy: 0.7993 - val_loss: 0.4758 - val_accuracy: 0.7831\n",
      "Epoch 4/25\n",
      "45/45 - 2s - loss: 0.2141 - accuracy: 0.9245 - val_loss: 0.5687 - val_accuracy: 0.7789\n",
      "Epoch 5/25\n",
      "45/45 - 2s - loss: 0.1039 - accuracy: 0.9672 - val_loss: 0.6458 - val_accuracy: 0.7689\n",
      "Epoch 6/25\n",
      "45/45 - 2s - loss: 0.0669 - accuracy: 0.9809 - val_loss: 0.7889 - val_accuracy: 0.7363\n",
      "Epoch 7/25\n",
      "45/45 - 2s - loss: 0.0497 - accuracy: 0.9856 - val_loss: 0.7733 - val_accuracy: 0.7558\n",
      "Epoch 8/25\n",
      "45/45 - 2s - loss: 0.0349 - accuracy: 0.9918 - val_loss: 0.9650 - val_accuracy: 0.7511\n",
      "Epoch 9/25\n",
      "45/45 - 2s - loss: 0.0262 - accuracy: 0.9937 - val_loss: 0.8397 - val_accuracy: 0.7605\n",
      "Epoch 10/25\n",
      "45/45 - 2s - loss: 0.0308 - accuracy: 0.9926 - val_loss: 0.8498 - val_accuracy: 0.7574\n",
      "Epoch 11/25\n",
      "45/45 - 2s - loss: 0.0242 - accuracy: 0.9953 - val_loss: 0.8647 - val_accuracy: 0.7616\n",
      "Epoch 12/25\n",
      "45/45 - 2s - loss: 0.0240 - accuracy: 0.9946 - val_loss: 0.9537 - val_accuracy: 0.7421\n",
      "Epoch 13/25\n",
      "45/45 - 3s - loss: 0.0245 - accuracy: 0.9935 - val_loss: 0.9439 - val_accuracy: 0.7589\n",
      "Epoch 14/25\n",
      "45/45 - 2s - loss: 0.0204 - accuracy: 0.9954 - val_loss: 1.0061 - val_accuracy: 0.7264\n",
      "Epoch 15/25\n",
      "45/45 - 2s - loss: 0.0210 - accuracy: 0.9949 - val_loss: 0.9512 - val_accuracy: 0.7463\n",
      "Epoch 16/25\n",
      "45/45 - 2s - loss: 0.0188 - accuracy: 0.9949 - val_loss: 1.1162 - val_accuracy: 0.7295\n",
      "Epoch 17/25\n",
      "45/45 - 2s - loss: 0.0163 - accuracy: 0.9958 - val_loss: 1.0794 - val_accuracy: 0.7390\n",
      "Epoch 18/25\n",
      "45/45 - 2s - loss: 0.0189 - accuracy: 0.9956 - val_loss: 0.9094 - val_accuracy: 0.7658\n",
      "Epoch 19/25\n",
      "45/45 - 2s - loss: 0.0171 - accuracy: 0.9954 - val_loss: 0.9105 - val_accuracy: 0.7700\n",
      "Epoch 20/25\n",
      "45/45 - 2s - loss: 0.0140 - accuracy: 0.9963 - val_loss: 0.9873 - val_accuracy: 0.7516\n",
      "Epoch 21/25\n",
      "45/45 - 2s - loss: 0.0142 - accuracy: 0.9960 - val_loss: 1.0066 - val_accuracy: 0.7537\n",
      "Epoch 22/25\n",
      "45/45 - 2s - loss: 0.0137 - accuracy: 0.9953 - val_loss: 0.9117 - val_accuracy: 0.7463\n",
      "Epoch 23/25\n",
      "45/45 - 2s - loss: 0.0129 - accuracy: 0.9963 - val_loss: 0.9548 - val_accuracy: 0.7489\n",
      "Epoch 24/25\n",
      "45/45 - 2s - loss: 0.0142 - accuracy: 0.9953 - val_loss: 0.9145 - val_accuracy: 0.7505\n",
      "Epoch 25/25\n",
      "45/45 - 2s - loss: 0.0139 - accuracy: 0.9947 - val_loss: 1.0155 - val_accuracy: 0.7495\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff2ecbb0f60>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_padded,y_train, batch_size=128, epochs=25, validation_data=(X_valid_padded,y_valid),verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predecimos con el set de test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tokens = tokenizer.texts_to_sequences(test_set.text)\n",
    "test_tokens_padded = pad_sequences(test_tokens, maxlen=max_len)\n",
    "\n",
    "predictions = model.predict(test_tokens_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.DataFrame(test_set['id'],columns=['id'])\n",
    "predictions = model.predict(test_tokens_padded)\n",
    "submit['target'] = predictions\n",
    "submit['target'] = round(submit['target']).astype('int')\n",
    "#submit.to_csv('SUBMITS/embeddings+DL.csv', index=False)\n",
    "\n",
    "#Con este, obtuvimos 0.73735 en kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a usar la implementacion Word2Vec de Gensim. Primero vamos a tokenizar los tweets (preprocesados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27496\n"
     ]
    }
   ],
   "source": [
    "tokens_list = list()\n",
    "for tweet in data_text.values:\n",
    "    tokens_list.append(word_tokenize(tweet))\n",
    "\n",
    "model2 = gensim.models.Word2Vec(sentences=tokens_list,size=EMBEDDING_SIZE,min_count=1)\n",
    "\n",
    "#cantidad de vocablos aprendidos\n",
    "tokens_generated = list(model2.wv.vocab)\n",
    "print(len(tokens_generated))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generacion de embeddings (usamos un set pre-entrenado de GloVe)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds = {}\n",
    "with open(os.environ['HOME']+'/glove.6B.100d.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], \"float32\")\n",
    "        embeds[word] = vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ahora, hay que convertir los embeddings en un vector de tokens.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de tokens unicos:  27496\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10876, 21)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer2 = Tokenizer()\n",
    "tokenizer2.fit_on_texts(tokens_list)\n",
    "sequences = tokenizer2.texts_to_sequences(tokens_list)\n",
    "\n",
    "#longitud para armar los textos con un pad para que tengan la misma longitud\n",
    "word_index = tokenizer2.word_index\n",
    "print(\"Cantidad de tokens unicos: \",len(word_index))\n",
    "\n",
    "tokens_padded = pad_sequences(sequences, maxlen=max_len)\n",
    "tokens_padded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mapear los embeddings del GloVe para cada palabra del vocabulario word_index y crear una matriz con esos vectores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "cant_tokens = len(word_index) + 1\n",
    "embeddings = np.zeros((cant_tokens,EMBEDDING_SIZE))\n",
    "vec = []\n",
    "for word, i in word_index.items():\n",
    "    if i > cant_tokens:\n",
    "        continue\n",
    "    try:\n",
    "       embeddings[i] = embeds[word]\n",
    "    except KeyError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27497, 100)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generar modelo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_16 (Embedding)     (None, 21, 100)           2749700   \n",
      "_________________________________________________________________\n",
      "gru_12 (GRU)                 (None, 32)                12864     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,762,597\n",
      "Trainable params: 12,897\n",
      "Non-trainable params: 2,749,700\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "embds_layer = Embedding(cant_tokens, EMBEDDING_SIZE, weights=[embeddings], input_length= max_len,\n",
    "              trainable=False)\n",
    "model2.add(embds_layer)\n",
    "model2.add(GRU(units=32,dropout=0.2, recurrent_dropout=0.2,return_sequences=False))\n",
    "model2.add(Dense(1, activation='sigmoid'))\n",
    "model2.compile(loss=\"binary_crossentropy\",optimizer='adam',metrics=['accuracy'])\n",
    "print(model2.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Como concatene los 2 datasets (train y test) para entrenar todos los embeddings, voy a separarlos de nuevo, y al de train lo vuelvo a separar en un set de validacion y uno de train.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train = tokens_padded[0:train_set.shape[0]]\n",
    "new_test = tokens_padded[train_set.shape[0]:]\n",
    "\n",
    "X_train2,X_valid2,y_train2,y_valid2 = train_test_split(new_train,train_set.target, test_size=0.25, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Entrenamos..**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "48/48 - 1s - loss: 0.6211 - accuracy: 0.6721 - val_loss: 0.5272 - val_accuracy: 0.7600\n",
      "Epoch 2/50\n",
      "48/48 - 1s - loss: 0.4889 - accuracy: 0.7807 - val_loss: 0.4525 - val_accuracy: 0.8020\n",
      "Epoch 3/50\n",
      "48/48 - 1s - loss: 0.4457 - accuracy: 0.8021 - val_loss: 0.4439 - val_accuracy: 0.8083\n",
      "Epoch 4/50\n",
      "48/48 - 1s - loss: 0.4375 - accuracy: 0.8066 - val_loss: 0.4410 - val_accuracy: 0.8072\n",
      "Epoch 5/50\n",
      "48/48 - 1s - loss: 0.4271 - accuracy: 0.8124 - val_loss: 0.4421 - val_accuracy: 0.8099\n",
      "Epoch 6/50\n",
      "48/48 - 1s - loss: 0.4197 - accuracy: 0.8142 - val_loss: 0.4379 - val_accuracy: 0.8041\n",
      "Epoch 7/50\n",
      "48/48 - 1s - loss: 0.4167 - accuracy: 0.8159 - val_loss: 0.4384 - val_accuracy: 0.8125\n",
      "Epoch 8/50\n",
      "48/48 - 1s - loss: 0.4143 - accuracy: 0.8191 - val_loss: 0.4359 - val_accuracy: 0.8093\n",
      "Epoch 9/50\n",
      "48/48 - 1s - loss: 0.4066 - accuracy: 0.8168 - val_loss: 0.4388 - val_accuracy: 0.8141\n",
      "Epoch 10/50\n",
      "48/48 - 1s - loss: 0.3999 - accuracy: 0.8240 - val_loss: 0.4411 - val_accuracy: 0.8136\n",
      "Epoch 11/50\n",
      "48/48 - 1s - loss: 0.3974 - accuracy: 0.8224 - val_loss: 0.4439 - val_accuracy: 0.8114\n",
      "Epoch 12/50\n",
      "48/48 - 1s - loss: 0.3937 - accuracy: 0.8238 - val_loss: 0.4388 - val_accuracy: 0.8162\n",
      "Epoch 13/50\n",
      "48/48 - 1s - loss: 0.3895 - accuracy: 0.8285 - val_loss: 0.4377 - val_accuracy: 0.8162\n",
      "Epoch 14/50\n",
      "48/48 - 1s - loss: 0.3852 - accuracy: 0.8301 - val_loss: 0.4371 - val_accuracy: 0.8146\n",
      "Epoch 15/50\n",
      "48/48 - 1s - loss: 0.3813 - accuracy: 0.8320 - val_loss: 0.4412 - val_accuracy: 0.8183\n",
      "Epoch 16/50\n",
      "48/48 - 1s - loss: 0.3738 - accuracy: 0.8390 - val_loss: 0.4425 - val_accuracy: 0.8114\n",
      "Epoch 17/50\n",
      "48/48 - 1s - loss: 0.3750 - accuracy: 0.8364 - val_loss: 0.4514 - val_accuracy: 0.8151\n",
      "Epoch 18/50\n",
      "48/48 - 1s - loss: 0.3708 - accuracy: 0.8382 - val_loss: 0.4381 - val_accuracy: 0.8114\n",
      "Epoch 19/50\n",
      "48/48 - 1s - loss: 0.3665 - accuracy: 0.8389 - val_loss: 0.4403 - val_accuracy: 0.8109\n",
      "Epoch 20/50\n",
      "48/48 - 1s - loss: 0.3647 - accuracy: 0.8432 - val_loss: 0.4436 - val_accuracy: 0.8109\n",
      "Epoch 21/50\n",
      "48/48 - 1s - loss: 0.3625 - accuracy: 0.8445 - val_loss: 0.4375 - val_accuracy: 0.8109\n",
      "Epoch 22/50\n",
      "48/48 - 1s - loss: 0.3503 - accuracy: 0.8495 - val_loss: 0.4499 - val_accuracy: 0.8114\n",
      "Epoch 23/50\n",
      "48/48 - 1s - loss: 0.3547 - accuracy: 0.8462 - val_loss: 0.4525 - val_accuracy: 0.8130\n",
      "Epoch 24/50\n",
      "48/48 - 1s - loss: 0.3451 - accuracy: 0.8508 - val_loss: 0.4476 - val_accuracy: 0.8078\n",
      "Epoch 25/50\n",
      "48/48 - 1s - loss: 0.3433 - accuracy: 0.8509 - val_loss: 0.4508 - val_accuracy: 0.8125\n",
      "Epoch 26/50\n",
      "48/48 - 1s - loss: 0.3418 - accuracy: 0.8485 - val_loss: 0.4526 - val_accuracy: 0.8104\n",
      "Epoch 27/50\n",
      "48/48 - 1s - loss: 0.3357 - accuracy: 0.8560 - val_loss: 0.4563 - val_accuracy: 0.8130\n",
      "Epoch 28/50\n",
      "48/48 - 1s - loss: 0.3322 - accuracy: 0.8583 - val_loss: 0.4566 - val_accuracy: 0.8088\n",
      "Epoch 29/50\n",
      "48/48 - 1s - loss: 0.3269 - accuracy: 0.8576 - val_loss: 0.4673 - val_accuracy: 0.7994\n",
      "Epoch 30/50\n",
      "48/48 - 1s - loss: 0.3219 - accuracy: 0.8623 - val_loss: 0.4641 - val_accuracy: 0.8114\n",
      "Epoch 31/50\n",
      "48/48 - 1s - loss: 0.3224 - accuracy: 0.8614 - val_loss: 0.4698 - val_accuracy: 0.8120\n",
      "Epoch 32/50\n",
      "48/48 - 1s - loss: 0.3208 - accuracy: 0.8623 - val_loss: 0.4706 - val_accuracy: 0.8030\n",
      "Epoch 33/50\n",
      "48/48 - 1s - loss: 0.3126 - accuracy: 0.8665 - val_loss: 0.4703 - val_accuracy: 0.8036\n",
      "Epoch 34/50\n",
      "48/48 - 1s - loss: 0.3128 - accuracy: 0.8695 - val_loss: 0.4764 - val_accuracy: 0.8072\n",
      "Epoch 35/50\n",
      "48/48 - 1s - loss: 0.3065 - accuracy: 0.8725 - val_loss: 0.4780 - val_accuracy: 0.8009\n",
      "Epoch 36/50\n",
      "48/48 - 1s - loss: 0.3024 - accuracy: 0.8716 - val_loss: 0.4718 - val_accuracy: 0.8062\n",
      "Epoch 37/50\n",
      "48/48 - 1s - loss: 0.2984 - accuracy: 0.8751 - val_loss: 0.4906 - val_accuracy: 0.8020\n",
      "Epoch 38/50\n",
      "48/48 - 1s - loss: 0.2961 - accuracy: 0.8718 - val_loss: 0.4830 - val_accuracy: 0.8030\n",
      "Epoch 39/50\n",
      "48/48 - 1s - loss: 0.2912 - accuracy: 0.8786 - val_loss: 0.4909 - val_accuracy: 0.8015\n",
      "Epoch 40/50\n",
      "48/48 - 1s - loss: 0.2987 - accuracy: 0.8762 - val_loss: 0.5042 - val_accuracy: 0.8009\n",
      "Epoch 41/50\n",
      "48/48 - 1s - loss: 0.2872 - accuracy: 0.8774 - val_loss: 0.4983 - val_accuracy: 0.8062\n",
      "Epoch 42/50\n",
      "48/48 - 1s - loss: 0.2886 - accuracy: 0.8779 - val_loss: 0.5012 - val_accuracy: 0.7994\n",
      "Epoch 43/50\n",
      "48/48 - 1s - loss: 0.2832 - accuracy: 0.8819 - val_loss: 0.5046 - val_accuracy: 0.8072\n",
      "Epoch 44/50\n",
      "48/48 - 1s - loss: 0.2832 - accuracy: 0.8793 - val_loss: 0.5051 - val_accuracy: 0.8041\n",
      "Epoch 45/50\n",
      "48/48 - 1s - loss: 0.2814 - accuracy: 0.8816 - val_loss: 0.5100 - val_accuracy: 0.8051\n",
      "Epoch 46/50\n",
      "48/48 - 1s - loss: 0.2730 - accuracy: 0.8846 - val_loss: 0.5036 - val_accuracy: 0.8093\n",
      "Epoch 47/50\n",
      "48/48 - 1s - loss: 0.2748 - accuracy: 0.8872 - val_loss: 0.4996 - val_accuracy: 0.8062\n",
      "Epoch 48/50\n",
      "48/48 - 1s - loss: 0.2758 - accuracy: 0.8833 - val_loss: 0.5128 - val_accuracy: 0.8036\n",
      "Epoch 49/50\n",
      "48/48 - 1s - loss: 0.2676 - accuracy: 0.8884 - val_loss: 0.5101 - val_accuracy: 0.8036\n",
      "Epoch 50/50\n",
      "48/48 - 1s - loss: 0.2690 - accuracy: 0.8865 - val_loss: 0.5178 - val_accuracy: 0.7973\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff1fc2c8860>"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(X_train2,y_train2, batch_size=120,epochs=50,validation_data=(X_valid2,y_valid2),verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predecimos..**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit2 = pd.DataFrame(test_set['id'],columns=['id'])\n",
    "predictions2 = model2.predict(new_test)\n",
    "submit2['target'] = predictions2\n",
    "submit2['target'] = round(submit2['target']).astype('int')\n",
    "#submit2.to_csv('SUBMITS/embeddings+DL-model2.csv', index=False)\n",
    "\n",
    "#Con este obtuvimos 0.80416 en KAGGLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo 3\n",
    "\n",
    "Usando los embeddings de Gensim y gloVe, agregando la columna 'keyword' como feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set-up, limpieza de datos y agregado de features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = pd.read_csv('test.csv')\n",
    "train_features = pd.read_csv('train.csv')\n",
    "\n",
    "train_features = train_features.fillna('')\n",
    "test_features = test_features.fillna('')\n",
    "\n",
    "for data in [test_features,train_features]:\n",
    "    #Pasar a minusculas, eliminar numeros y stopwords\n",
    "    data['text'] = data['text'].apply(lambda x: minusculas(x))\n",
    "    data['text'] = data['text'].apply(lambda x: eliminar_numeros(x))\n",
    "    data['text'] = data['text'].apply(lambda x: remove_stopwords(x))\n",
    "    data['keyword'] = data['keyword'].apply(lambda x: re.sub(r'%20',' ', str(x)))\n",
    "    #Agregado de features\n",
    "    data['combined_text'] = data['text']+' '+data['keyword']\n",
    "    data['qty_hashtags'] = data['text'].apply(lambda x: x.count('#'))\n",
    "    data['qty_urls'] = data['text'].apply(lambda x: x.count('http'))\n",
    "    data['tweet_len'] = data['text'].str.len()\n",
    "    data['qty_words_tweet'] = data['text'].apply(lambda x: len(str(x).split()))\n",
    "    data['len_diff_mean'] = abs(data['tweet_len'] - data['tweet_len'].mean())\n",
    "    \n",
    "X3 = train_features[['combined_text',\n",
    "                     'qty_hashtags',\n",
    "                     'qty_urls',\n",
    "                     'tweet_len',\n",
    "                     'qty_words_tweet',\n",
    "                     'len_diff_mean']]\n",
    "y3 = train_features.target\n",
    "\n",
    "#Concateno los dos dataframes para vectorizar todo junto\n",
    "data_text3 = pd.concat([X3,test_features[['combined_text',\n",
    "                                         'qty_hashtags',\n",
    "                                         'qty_urls',\n",
    "                                         'tweet_len',\n",
    "                                         'qty_words_tweet',\n",
    "                                         'len_diff_mean']]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenizacion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28524\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_SIZE = 100\n",
    "tokens_list3 = list()\n",
    "for tweet in data_text3.combined_text.values:\n",
    "    tokens_list3.append(word_tokenize(tweet))\n",
    "\n",
    "model3 = gensim.models.Word2Vec(sentences=tokens_list3,size=EMBEDDING_SIZE,min_count=1)\n",
    "\n",
    "#cantidad de vocablos aprendidos\n",
    "tokens_generated3 = list(model3.wv.vocab)\n",
    "print(len(tokens_generated3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Embeddings usando un set de GloVe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds3 = {}\n",
    "with open(os.environ['HOME']+'/glove.6B.100d.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], \"float32\")\n",
    "        embeds3[word] = vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convertir embeddings en tokens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de tokens unicos:  28524\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10876, 29)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer3 = Tokenizer()\n",
    "tokenizer3.fit_on_texts(tokens_list3)\n",
    "sequences3 = tokenizer3.texts_to_sequences(tokens_list3)\n",
    "\n",
    "#longitud para armar los textos con un pad para que tengan la misma longitud\n",
    "max_len3 = max([len(text.split()) for text in data_text3.combined_text])\n",
    "\n",
    "#longitud para armar los textos con un pad para que tengan la misma longitud\n",
    "word_index3 = tokenizer3.word_index\n",
    "print(\"Cantidad de tokens unicos: \",len(word_index3))\n",
    "\n",
    "tokens_padded3 = pad_sequences(sequences3, maxlen=max_len3)\n",
    "tokens_padded3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mapear los embeddings del GloVe para cada palabra del vocabulario word_index y crear una matriz con esos vectores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension de la matriz de embeds:  (28525, 100)\n"
     ]
    }
   ],
   "source": [
    "cant_tokens3 = len(word_index3) + 1\n",
    "embeddings3 = np.zeros((cant_tokens3,EMBEDDING_SIZE))\n",
    "vec = []\n",
    "for word, i in word_index3.items():\n",
    "    if i > cant_tokens3:\n",
    "        continue\n",
    "    try:\n",
    "       embeddings3[i] = embeds3[word]\n",
    "    except KeyError:\n",
    "        continue\n",
    "        \n",
    "print(\"Dimension de la matriz de embeds: \",embeddings3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Como concatene los 2 datasets (train y test) para entrenar todos los embeddings, voy a separarlos de nuevo, y al de train lo vuelvo a separar en un set de validacion y uno de train.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7613, 34)\n",
      "(3263, 34)\n"
     ]
    }
   ],
   "source": [
    "#Convierto a dataframe los vectores tokenizados para poder\n",
    "#agregar features numericos\n",
    "new_train3 = pd.DataFrame(tokens_padded3[0:train_features.shape[0]])\n",
    "new_test3 = pd.DataFrame(tokens_padded3[train_features.shape[0]:])\n",
    "\n",
    "#Agrego los features numericos\n",
    "new_train3['qty_hashtags'] = X3['qty_hashtags']\n",
    "new_train3['qty_urls'] = X3['qty_urls']\n",
    "new_train3['tweet_len'] = X3['tweet_len']\n",
    "new_train3['qty_words_tweet'] = X3['qty_words_tweet']\n",
    "new_train3['len_diff_mean'] = X3['len_diff_mean']\n",
    "\n",
    "new_test3['qty_hashtags'] = test_features['qty_hashtags']\n",
    "new_test3['qty_urls'] = test_features['qty_urls']\n",
    "new_test3['tweet_len'] = test_features['tweet_len']\n",
    "new_test3['qty_words_tweet'] = test_features['qty_words_tweet']\n",
    "new_test3['len_diff_mean'] = test_features['len_diff_mean']\n",
    "\n",
    "#Divido el set de train en 75% para entrenar y 25% para validacion\n",
    "X_train3,X_valid3,y_train3,y_valid3 = train_test_split(new_train3,train_features.target, test_size=0.25, random_state=1)\n",
    "\n",
    "print(new_train3.shape)\n",
    "print(new_test3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generar modelo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_17 (Embedding)     (None, 34, 100)           2852500   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 32)                17024     \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,869,557\n",
      "Trainable params: 17,057\n",
      "Non-trainable params: 2,852,500\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model3 = Sequential()\n",
    "embds_layer3 = Embedding(cant_tokens3, EMBEDDING_SIZE, weights=[embeddings3], input_length= max_len3+5,\n",
    "              trainable=False)\n",
    "model3.add(embds_layer3)\n",
    "model3.add(LSTM(units=32,dropout=0.2, recurrent_dropout=0.2,return_sequences=False))\n",
    "model3.add(Dense(1, activation='sigmoid'))\n",
    "model3.compile(loss=\"binary_crossentropy\",optimizer='adam',metrics=['accuracy'])\n",
    "print(model3.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Entrenamiento**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "45/45 - 2s - loss: 0.6283 - accuracy: 0.6434 - val_loss: 0.5232 - val_accuracy: 0.7642\n",
      "Epoch 2/50\n",
      "45/45 - 2s - loss: 0.5062 - accuracy: 0.7670 - val_loss: 0.4819 - val_accuracy: 0.7868\n",
      "Epoch 3/50\n",
      "45/45 - 2s - loss: 0.4659 - accuracy: 0.7944 - val_loss: 0.4497 - val_accuracy: 0.8057\n",
      "Epoch 4/50\n",
      "45/45 - 2s - loss: 0.4497 - accuracy: 0.7998 - val_loss: 0.4436 - val_accuracy: 0.8009\n",
      "Epoch 5/50\n",
      "45/45 - 2s - loss: 0.4374 - accuracy: 0.8033 - val_loss: 0.4393 - val_accuracy: 0.8104\n",
      "Epoch 6/50\n",
      "45/45 - 2s - loss: 0.4276 - accuracy: 0.8085 - val_loss: 0.4403 - val_accuracy: 0.8030\n",
      "Epoch 7/50\n",
      "45/45 - 2s - loss: 0.4159 - accuracy: 0.8185 - val_loss: 0.4453 - val_accuracy: 0.8104\n",
      "Epoch 8/50\n",
      "45/45 - 2s - loss: 0.4090 - accuracy: 0.8166 - val_loss: 0.4513 - val_accuracy: 0.8078\n",
      "Epoch 9/50\n",
      "45/45 - 2s - loss: 0.4071 - accuracy: 0.8241 - val_loss: 0.4487 - val_accuracy: 0.8114\n",
      "Epoch 10/50\n",
      "45/45 - 2s - loss: 0.4030 - accuracy: 0.8210 - val_loss: 0.4505 - val_accuracy: 0.8078\n",
      "Epoch 11/50\n",
      "45/45 - 2s - loss: 0.3908 - accuracy: 0.8306 - val_loss: 0.4837 - val_accuracy: 0.7946\n",
      "Epoch 12/50\n",
      "45/45 - 2s - loss: 0.3920 - accuracy: 0.8283 - val_loss: 0.4488 - val_accuracy: 0.8109\n",
      "Epoch 13/50\n",
      "45/45 - 2s - loss: 0.3856 - accuracy: 0.8359 - val_loss: 0.4668 - val_accuracy: 0.8078\n",
      "Epoch 14/50\n",
      "45/45 - 2s - loss: 0.3816 - accuracy: 0.8318 - val_loss: 0.4520 - val_accuracy: 0.8114\n",
      "Epoch 15/50\n",
      "45/45 - 2s - loss: 0.3760 - accuracy: 0.8382 - val_loss: 0.4489 - val_accuracy: 0.8109\n",
      "Epoch 16/50\n",
      "45/45 - 2s - loss: 0.3728 - accuracy: 0.8362 - val_loss: 0.4685 - val_accuracy: 0.8109\n",
      "Epoch 17/50\n",
      "45/45 - 2s - loss: 0.3699 - accuracy: 0.8411 - val_loss: 0.4805 - val_accuracy: 0.8009\n",
      "Epoch 18/50\n",
      "45/45 - 2s - loss: 0.3686 - accuracy: 0.8390 - val_loss: 0.4440 - val_accuracy: 0.8136\n",
      "Epoch 19/50\n",
      "45/45 - 2s - loss: 0.3612 - accuracy: 0.8462 - val_loss: 0.4579 - val_accuracy: 0.8146\n",
      "Epoch 20/50\n",
      "45/45 - 2s - loss: 0.3558 - accuracy: 0.8494 - val_loss: 0.4548 - val_accuracy: 0.8099\n",
      "Epoch 21/50\n",
      "45/45 - 2s - loss: 0.3527 - accuracy: 0.8490 - val_loss: 0.4561 - val_accuracy: 0.8125\n",
      "Epoch 22/50\n",
      "45/45 - 2s - loss: 0.3453 - accuracy: 0.8481 - val_loss: 0.4593 - val_accuracy: 0.8083\n",
      "Epoch 23/50\n",
      "45/45 - 2s - loss: 0.3383 - accuracy: 0.8558 - val_loss: 0.4712 - val_accuracy: 0.8120\n",
      "Epoch 24/50\n",
      "45/45 - 2s - loss: 0.3392 - accuracy: 0.8593 - val_loss: 0.4596 - val_accuracy: 0.8093\n",
      "Epoch 25/50\n",
      "45/45 - 2s - loss: 0.3416 - accuracy: 0.8525 - val_loss: 0.4563 - val_accuracy: 0.8093\n",
      "Epoch 26/50\n",
      "45/45 - 2s - loss: 0.3339 - accuracy: 0.8543 - val_loss: 0.4741 - val_accuracy: 0.8114\n",
      "Epoch 27/50\n",
      "45/45 - 2s - loss: 0.3300 - accuracy: 0.8600 - val_loss: 0.4677 - val_accuracy: 0.8078\n",
      "Epoch 28/50\n",
      "45/45 - 2s - loss: 0.3260 - accuracy: 0.8635 - val_loss: 0.4707 - val_accuracy: 0.8088\n",
      "Epoch 29/50\n",
      "45/45 - 2s - loss: 0.3220 - accuracy: 0.8700 - val_loss: 0.4692 - val_accuracy: 0.8109\n",
      "Epoch 30/50\n",
      "45/45 - 2s - loss: 0.3207 - accuracy: 0.8642 - val_loss: 0.4585 - val_accuracy: 0.8093\n",
      "Epoch 31/50\n",
      "45/45 - 2s - loss: 0.3136 - accuracy: 0.8728 - val_loss: 0.4768 - val_accuracy: 0.8088\n",
      "Epoch 32/50\n",
      "45/45 - 2s - loss: 0.3084 - accuracy: 0.8727 - val_loss: 0.4873 - val_accuracy: 0.8114\n",
      "Epoch 33/50\n",
      "45/45 - 2s - loss: 0.3076 - accuracy: 0.8695 - val_loss: 0.5088 - val_accuracy: 0.8078\n",
      "Epoch 34/50\n",
      "45/45 - 2s - loss: 0.2990 - accuracy: 0.8755 - val_loss: 0.5005 - val_accuracy: 0.8078\n",
      "Epoch 35/50\n",
      "45/45 - 2s - loss: 0.2955 - accuracy: 0.8777 - val_loss: 0.4786 - val_accuracy: 0.8051\n",
      "Epoch 36/50\n",
      "45/45 - 2s - loss: 0.2955 - accuracy: 0.8816 - val_loss: 0.4939 - val_accuracy: 0.8125\n",
      "Epoch 37/50\n",
      "45/45 - 2s - loss: 0.2922 - accuracy: 0.8791 - val_loss: 0.4939 - val_accuracy: 0.8125\n",
      "Epoch 38/50\n",
      "45/45 - 2s - loss: 0.2961 - accuracy: 0.8742 - val_loss: 0.4996 - val_accuracy: 0.8078\n",
      "Epoch 39/50\n",
      "45/45 - 2s - loss: 0.2853 - accuracy: 0.8828 - val_loss: 0.5048 - val_accuracy: 0.8099\n",
      "Epoch 40/50\n",
      "45/45 - 2s - loss: 0.2833 - accuracy: 0.8860 - val_loss: 0.5080 - val_accuracy: 0.8062\n",
      "Epoch 41/50\n",
      "45/45 - 2s - loss: 0.2806 - accuracy: 0.8874 - val_loss: 0.5304 - val_accuracy: 0.8104\n",
      "Epoch 42/50\n",
      "45/45 - 2s - loss: 0.2801 - accuracy: 0.8839 - val_loss: 0.5202 - val_accuracy: 0.8036\n",
      "Epoch 43/50\n",
      "45/45 - 2s - loss: 0.2703 - accuracy: 0.8895 - val_loss: 0.5346 - val_accuracy: 0.8051\n",
      "Epoch 44/50\n",
      "45/45 - 2s - loss: 0.2700 - accuracy: 0.8900 - val_loss: 0.5144 - val_accuracy: 0.8046\n",
      "Epoch 45/50\n",
      "45/45 - 2s - loss: 0.2639 - accuracy: 0.8921 - val_loss: 0.5412 - val_accuracy: 0.8067\n",
      "Epoch 46/50\n",
      "45/45 - 2s - loss: 0.2684 - accuracy: 0.8907 - val_loss: 0.5208 - val_accuracy: 0.7978\n",
      "Epoch 47/50\n",
      "45/45 - 2s - loss: 0.2606 - accuracy: 0.8937 - val_loss: 0.5252 - val_accuracy: 0.8099\n",
      "Epoch 48/50\n",
      "45/45 - 2s - loss: 0.2568 - accuracy: 0.8986 - val_loss: 0.5592 - val_accuracy: 0.8057\n",
      "Epoch 49/50\n",
      "45/45 - 2s - loss: 0.2632 - accuracy: 0.8905 - val_loss: 0.5384 - val_accuracy: 0.7994\n",
      "Epoch 50/50\n",
      "45/45 - 2s - loss: 0.2572 - accuracy: 0.8939 - val_loss: 0.5595 - val_accuracy: 0.7925\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb523ad4160>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.fit(X_train3,y_train3,batch_size=128,epochs=50,validation_data=(X_valid3,y_valid3),verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predicción**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit3 = pd.DataFrame(test_features['id'],columns=['id'])\n",
    "predictions3 = model3.predict(new_test3)\n",
    "submit3['target'] = predictions3\n",
    "submit3['target'] = round(submit3['target']).astype('int')\n",
    "submit3.to_csv('SUBMITS/embeddings+DL-model3.csv', index=False)\n",
    "\n",
    "#Con este obtuvimos 0.79436 en KAGGLE, peor que sin usar features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
