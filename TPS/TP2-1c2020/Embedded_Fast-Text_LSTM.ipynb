{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importo librer√≠as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from simpletransformers.classification import ClassificationModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tqdm import tqdm\n",
    "\n",
    "import emoji\n",
    "import itertools\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import os, re, csv, math, codecs\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "MAX_NB_WORDS = 100000\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importo word embeddings de wikipedia Fast Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "999995it [01:42, 9760.35it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 999995 word vectors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "embeddings_index = {}\n",
    "f = codecs.open('wiki-news-300d-1M.vec', encoding='utf-8')\n",
    "for line in tqdm(f):\n",
    "    values = line.rstrip().rsplit(' ')\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('found %s word vectors' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leo los csvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape train:  7613\n",
      "shape test:  3263\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_df = pd.read_csv('train.csv', sep=',', header=0)\n",
    "test_df = pd.read_csv('test.csv', sep=',', header=0)\n",
    "test_df = test_df.fillna('_NA_')\n",
    "\n",
    "\n",
    "print(\"shape train: \", train_df.shape[0])\n",
    "print(\"shape test: \", test_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## limpio los datasets y  les remuevo contracciones del lenguaje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reemplazo las contracciones del lenguaje\n",
    "\n",
    "contractions = { \n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"needn't\": \"need not\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there had\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who's\": \"who is\",\n",
    "\"won't\": \"will not\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you're\": \"you are\",\n",
    "\"thx\"   : \"thanks\"\n",
    "}\n",
    "\n",
    "\n",
    "def remove_contractions(text):\n",
    "    return contractions[text.lower()] if text.lower() in contractions.keys() else text\n",
    "\n",
    "train_df['text']=train_df['text'].apply(remove_contractions)\n",
    "test_df['text']=test_df['text'].apply(remove_contractions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataset(text):\n",
    "    # Remove hashtag while keeping hashtag text\n",
    "    text = re.sub(r'#','', text)\n",
    "    # Remove HTML special entities (e.g. &amp;)\n",
    "    text = re.sub(r'\\&\\w*;', '', text)\n",
    "    # Remove tickers\n",
    "    text = re.sub(r'\\$\\w*', '', text)\n",
    "    # Remove hyperlinks\n",
    "    text = re.sub(r'https?:\\/\\/.*\\/\\w*', '', text)\n",
    "    # Remove whitespace (including new line characters)\n",
    "    text = re.sub(r'\\s\\s+','', text)\n",
    "    text = re.sub(r'[ ]{2, }',' ',text)\n",
    "    # Remove URL, RT, mention(@)\n",
    "    text=  re.sub(r'http(\\S)+', '',text)\n",
    "    text=  re.sub(r'http ...', '',text)\n",
    "    text=  re.sub(r'(RT|rt)[ ]*@[ ]*[\\S]+','',text)\n",
    "    text=  re.sub(r'RT[ ]?@','',text)\n",
    "    text = re.sub(r'@[\\S]+','',text)\n",
    "    # Remove words with 4 or fewer letters\n",
    "    text = re.sub(r'\\b\\w{1,4}\\b', '', text)\n",
    "    #&, < and >\n",
    "    text = re.sub(r'&amp;?', 'and',text)\n",
    "    text = re.sub(r'&lt;','<',text)\n",
    "    text = re.sub(r'&gt;','>',text)\n",
    "    # Remove characters beyond Basic Multilingual Plane (BMP) of Unicode:\n",
    "    text= ''.join(c for c in text if c <= '\\uFFFF') \n",
    "    text = text.strip()\n",
    "    # Remove misspelling words\n",
    "    text = ''.join(''.join(s)[:2] for _, s in itertools.groupby(text))\n",
    "    # Remove emoji\n",
    "    text = emoji.demojize(text)\n",
    "    text = text.replace(\":\",\" \")\n",
    "    text = ' '.join(text.split()) \n",
    "    text = re.sub(\"([^\\x00-\\x7F])+\",\" \",text)\n",
    "    # Remove Mojibake (also extra spaces)\n",
    "    text = ' '.join(re.sub(\"[^\\u4e00-\\u9fa5\\u0030-\\u0039\\u0041-\\u005a\\u0061-\\u007a]\", \" \", text).split())\n",
    "    # Remove stopwords\n",
    "    pattern = re.compile(r'\\b(' + r'|'.join(stopwords.words('english')) + r')\\b\\s*')\n",
    "    text = pattern.sub('', text)\n",
    "    return text\n",
    "\n",
    "train_df['text']=train_df['text'].apply(clean_dataset)\n",
    "test_df['text']=test_df['text'].apply(clean_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizing input data...\n",
      "dictionary size:  11788\n"
     ]
    }
   ],
   "source": [
    "label_names = [\"target\"]\n",
    "y_train = train_df[label_names].values\n",
    "train_df['doc_len'] = train_df['text'].apply(lambda words: len(words.split(\" \")))\n",
    "max_seq_len = np.round(train_df['doc_len'].mean() + train_df['doc_len'].std()).astype(int)\n",
    "\n",
    "processed_docs_train = train_df['text'].tolist()\n",
    "processed_docs_test = test_df['text'].tolist() \n",
    "num_classes = len(label_names)\n",
    "\n",
    "print(\"tokenizing input data...\")\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(processed_docs_train)  \n",
    "word_seq_train = tokenizer.texts_to_sequences(processed_docs_train)\n",
    "word_seq_test = tokenizer.texts_to_sequences(processed_docs_test)\n",
    "word_index = tokenizer.word_index\n",
    "print(\"dictionary size: \", len(word_index))\n",
    "\n",
    "#pad sequences\n",
    "word_seq_train = sequence.pad_sequences(word_seq_train, maxlen=max_seq_len)\n",
    "word_seq_test = sequence.pad_sequences(word_seq_test, maxlen=max_seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entreno "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training params\n",
    "batch_size = 256 \n",
    "num_epochs = 50\n",
    "\n",
    "#model parameters\n",
    "num_filters = 64 \n",
    "embed_dim = 300 \n",
    "weight_decay = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 9, 300)            3536700   \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 9, 300)            0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 9, 32)             9632      \n",
      "_________________________________________________________________\n",
      "bidirectional_10 (Bidirectio (None, 9, 128)            49664     \n",
      "_________________________________________________________________\n",
      "bidirectional_11 (Bidirectio (None, 64)                41216     \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 3,639,325\n",
      "Trainable params: 102,625\n",
      "Non-trainable params: 3,536,700\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "21/21 [==============================] - 2s 98ms/step - loss: 0.6662 - accuracy: 0.6117 - val_loss: 0.5786 - val_accuracy: 0.7386\n",
      "Epoch 2/50\n",
      "21/21 [==============================] - 1s 42ms/step - loss: 0.5466 - accuracy: 0.7365 - val_loss: 0.4636 - val_accuracy: 0.7995\n",
      "Epoch 3/50\n",
      "21/21 [==============================] - 1s 42ms/step - loss: 0.5017 - accuracy: 0.7726 - val_loss: 0.4487 - val_accuracy: 0.7960\n",
      "Epoch 4/50\n",
      "21/21 [==============================] - 1s 45ms/step - loss: 0.4876 - accuracy: 0.7739 - val_loss: 0.4452 - val_accuracy: 0.8025\n",
      "Epoch 5/50\n",
      "21/21 [==============================] - 1s 46ms/step - loss: 0.4737 - accuracy: 0.7840 - val_loss: 0.4376 - val_accuracy: 0.8017\n",
      "Epoch 6/50\n",
      "21/21 [==============================] - 1s 44ms/step - loss: 0.4664 - accuracy: 0.7923 - val_loss: 0.4370 - val_accuracy: 0.8012\n",
      "Epoch 7/50\n",
      "21/21 [==============================] - 1s 45ms/step - loss: 0.4594 - accuracy: 0.7943 - val_loss: 0.4447 - val_accuracy: 0.7929\n",
      "Epoch 8/50\n",
      "21/21 [==============================] - 1s 44ms/step - loss: 0.4567 - accuracy: 0.8002 - val_loss: 0.4369 - val_accuracy: 0.7982\n",
      "Epoch 9/50\n",
      "21/21 [==============================] - 1s 44ms/step - loss: 0.4526 - accuracy: 0.8009 - val_loss: 0.4413 - val_accuracy: 0.7990\n",
      "Epoch 10/50\n",
      "21/21 [==============================] - 1s 43ms/step - loss: 0.4471 - accuracy: 0.8041 - val_loss: 0.4352 - val_accuracy: 0.8043\n",
      "Epoch 11/50\n",
      "21/21 [==============================] - 1s 42ms/step - loss: 0.4367 - accuracy: 0.8108 - val_loss: 0.4336 - val_accuracy: 0.8030\n",
      "Epoch 12/50\n",
      "21/21 [==============================] - 1s 42ms/step - loss: 0.4361 - accuracy: 0.8065 - val_loss: 0.4479 - val_accuracy: 0.7951\n",
      "Epoch 13/50\n",
      "21/21 [==============================] - 1s 42ms/step - loss: 0.4336 - accuracy: 0.8116 - val_loss: 0.4764 - val_accuracy: 0.7877\n",
      "Epoch 14/50\n",
      "21/21 [==============================] - 1s 42ms/step - loss: 0.4252 - accuracy: 0.8138 - val_loss: 0.4499 - val_accuracy: 0.7951\n"
     ]
    }
   ],
   "source": [
    "#embedding matrix\n",
    "words_not_found = []\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index)+1)\n",
    "embedding_matrix = np.zeros((nb_words, embed_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i >= nb_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        words_not_found.append(word)\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(Embedding(nb_words,embed_dim,input_length=max_seq_len, weights=[embedding_matrix],trainable=False))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(32,activation='relu'))\n",
    "model.add(Bidirectional(LSTM(64,return_sequences= True)))\n",
    "model.add(Bidirectional(LSTM(32)))\n",
    "model.add(Dense(32,activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "es_callback = EarlyStopping(monitor='val_loss', patience=3)\n",
    "history = model.fit(word_seq_train, y_train, batch_size=256,\n",
    "          epochs=num_epochs, validation_split=0.3, callbacks=[es_callback], shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions= (model.predict(word_seq_test) >0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample=pd.read_csv('test.csv')\n",
    "sample['target']= predictions\n",
    "sample[['id', 'target']].to_csv('FastText-TensorFlow.csv',index= None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
